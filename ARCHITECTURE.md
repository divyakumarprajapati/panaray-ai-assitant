# ğŸ—ï¸ Architecture Documentation

## System Overview

The PANARAY Feature Assistant is a full-stack RAG (Retrieval-Augmented Generation) system with emotion detection and adaptive responses.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         User Browser                            â”‚
â”‚                   (React + TypeScript + Tailwind)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚ HTTP/JSON
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     FastAPI Backend                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                    API Layer                             â”‚  â”‚
â”‚  â”‚  â€¢ /api/query     â€¢ /api/health    â€¢ /api/index         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                         â”‚                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                  RAG Service (Orchestrator)              â”‚  â”‚
â”‚  â””â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”˜  â”‚
â”‚     â”‚     â”‚     â”‚      â”‚                                  â”‚     â”‚
â”‚     â–¼     â–¼     â–¼      â–¼                                  â–¼     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”                          â”Œâ”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚Emb.â”‚â”‚Emo.â”‚â”‚LLM â”‚â”‚Vectorâ”‚                          â”‚Data  â”‚ â”‚
â”‚  â”‚Svc â”‚â”‚Svc â”‚â”‚Svc â”‚â”‚Svc   â”‚                          â”‚Loaderâ”‚ â”‚
â”‚  â””â”€â”¬â”€â”€â”˜â””â”€â”¬â”€â”€â”˜â””â”€â”¬â”€â”€â”˜â””â”€â”€â”¬â”€â”€â”€â”˜                          â””â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚     â”‚     â”‚      â”‚
     â–¼     â–¼     â–¼      â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚        External Services                   â”‚
  â”‚  â€¢ Hugging Face API (LLM + Emotion)       â”‚
  â”‚  â€¢ Sentence Transformers (Embeddings)     â”‚
  â”‚  â€¢ Pinecone (Vector Database)             â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Component Architecture

### Backend Architecture (SOLID Design)

#### 1. Service Layer

Each service follows the **Single Responsibility Principle**:

**EmbeddingService** (`embedding_service.py`)
```
Responsibility: Generate vector embeddings
- Uses: sentence-transformers/all-MiniLM-L6-v2
- Methods: generate_embedding(), generate_embeddings_batch()
- Output: 384-dimensional vectors
```

**EmotionService** (`emotion_service.py`)
```
Responsibility: Detect emotions from text
- Uses: distilbert-base-uncased-emotion
- Methods: detect_emotion(), get_tone_for_emotion()
- Output: Emotion label + confidence score
- Maps emotions to response tones
```

**LLMService** (`llm_service.py`)
```
Responsibility: Generate responses using LLM
- Uses: Llama 3 via Hugging Face Inference API
- Methods: generate_response(), _build_prompt()
- Features: Context injection, tone adaptation
```

**VectorService** (`vector_service.py`)
```
Responsibility: Vector database operations
- Uses: Pinecone serverless
- Methods: upsert_vectors(), query_similar(), get_stats()
- Features: Batch operations, similarity search
```

**RAGService** (`rag_service.py`)
```
Responsibility: Orchestrate the RAG pipeline
- Depends on: All other services (Dependency Injection)
- Methods: process_query()
- Pipeline:
  1. Detect emotion
  2. Generate query embedding
  3. Retrieve similar documents
  4. Generate response with adapted tone
```

#### 2. API Layer

**Routes** (`api/routes.py`)
```
Endpoints:
- GET  /api/health     â†’ Health check
- POST /api/query      â†’ Process user query
- POST /api/index      â†’ Index/reindex data
```

#### 3. Data Layer

**Models** (`models/schemas.py`)
```
Pydantic v2 schemas for validation:
- QueryRequest, QueryResponse
- EmotionResult
- HealthResponse, IndexDataResponse
```

**Configuration** (`config.py`)
```
Environment-based configuration:
- API keys (Hugging Face, Pinecone)
- Model names
- RAG parameters (top_k, similarity_threshold)
```

**Data Loader** (`utils/data_loader.py`)
```
Loads and prepares JSONL data:
- Parses conversation format
- Combines Q&A for better retrieval
- Prepares metadata
```

### Frontend Architecture

#### Component Hierarchy

```
App.tsx
â”œâ”€â”€ Header
â”‚   â”œâ”€â”€ Title
â”‚   â””â”€â”€ HealthStatus
â”‚
â”œâ”€â”€ ErrorBanner (conditional)
â”‚
â””â”€â”€ ChatContainer
    â”œâ”€â”€ ChatHeader
    â”‚   â”œâ”€â”€ Title with Icon
    â”‚   â””â”€â”€ ClearButton
    â”‚
    â”œâ”€â”€ ScrollArea (Messages)
    â”‚   â””â”€â”€ ChatMessage[] (mapped)
    â”‚       â”œâ”€â”€ Avatar (User/Bot)
    â”‚       â”œâ”€â”€ Content
    â”‚       â””â”€â”€ Metadata (emotion, sources, confidence)
    â”‚
    â””â”€â”€ ChatInput
        â”œâ”€â”€ Input
        â””â”€â”€ SendButton
```

#### State Management

**useChat Hook** (`hooks/useChat.ts`)
```typescript
Manages chat state:
- messages: Message[]
- isLoading: boolean
- error: string | null

Methods:
- sendMessage(content: string)
- clearMessages()

Flow:
1. Add user message to state
2. Call API via queryAssistant()
3. Add assistant response to state
4. Handle errors gracefully
```

#### API Service

**api.ts** (`services/api.ts`)
```typescript
HTTP client for backend:
- queryAssistant(query)
- checkHealth()
- indexData(forceReindex)

Features:
- Type-safe responses
- Error handling
- Configurable base URL
```

#### UI Components

Built with **Radix UI** primitives for accessibility:

- **Button**: Multiple variants (default, secondary, outline, ghost)
- **Card**: Container with header, title, content
- **Input**: Text input with focus states
- **ScrollArea**: Smooth scrolling with custom scrollbar

#### Styling

**Tailwind CSS** with custom theme:
- CSS variables for colors
- Consistent spacing and typography
- Responsive design
- Dark mode ready (variables defined)

## Data Flow

### Query Processing Flow

```
1. User Input
   â”‚
   â”œâ”€â†’ Frontend: ChatInput.tsx
   â”‚
   â”œâ”€â†’ Hook: useChat.ts
   â”‚   â””â”€â†’ Add user message to state
   â”‚
   â”œâ”€â†’ API: services/api.ts
   â”‚   â””â”€â†’ POST /api/query
   â”‚
   â”œâ”€â†’ Backend: routes.py
   â”‚   â””â”€â†’ Validate with Pydantic
   â”‚
   â”œâ”€â†’ RAGService: rag_service.py
   â”‚   â”‚
   â”‚   â”œâ”€â†’ EmotionService
   â”‚   â”‚   â””â”€â†’ Detect emotion + confidence
   â”‚   â”‚
   â”‚   â”œâ”€â†’ EmbeddingService
   â”‚   â”‚   â””â”€â†’ Generate query vector
   â”‚   â”‚
   â”‚   â”œâ”€â†’ VectorService
   â”‚   â”‚   â””â”€â†’ Query Pinecone for similar docs
   â”‚   â”‚
   â”‚   â””â”€â†’ LLMService
   â”‚       â””â”€â†’ Generate response with context + tone
   â”‚
   â”œâ”€â†’ Response: QueryResponse
   â”‚   â”œâ”€â†’ answer: string
   â”‚   â”œâ”€â†’ emotion: EmotionResult
   â”‚   â”œâ”€â†’ sources_used: int
   â”‚   â””â”€â†’ confidence: float
   â”‚
   â”œâ”€â†’ Frontend: useChat.ts
   â”‚   â””â”€â†’ Add assistant message to state
   â”‚
   â””â”€â†’ UI: ChatMessage.tsx
       â””â”€â†’ Display with metadata
```

### Indexing Flow

```
1. Index Request
   â”‚
   â”œâ”€â†’ POST /api/index
   â”‚
   â”œâ”€â†’ DataLoader
   â”‚   â””â”€â†’ Load features.jsonl
   â”‚
   â”œâ”€â†’ EmbeddingService
   â”‚   â””â”€â†’ Generate embeddings for all docs
   â”‚
   â”œâ”€â†’ VectorService
   â”‚   â””â”€â†’ Upsert to Pinecone
   â”‚       â”œâ”€â†’ vectors: List[float]
   â”‚       â”œâ”€â†’ metadata: { content, question, answer }
   â”‚       â””â”€â†’ ids: doc_0, doc_1, ...
   â”‚
   â””â”€â†’ Response: indexed_count
```

## Design Patterns

### 1. Dependency Injection

RAGService receives all dependencies in constructor:

```python
def __init__(
    self,
    embedding_service: EmbeddingService,
    emotion_service: EmotionService,
    llm_service: LLMService,
    vector_service: VectorService
)
```

Benefits:
- Testable (can mock services)
- Flexible (can swap implementations)
- Clear dependencies

### 2. Service Locator

API routes use a `get_services()` dependency:

```python
@router.post("/query")
async def query_assistant(
    request: QueryRequest,
    services: Annotated[dict, Depends(get_services)]
)
```

Benefits:
- Singleton services (created once)
- Lazy initialization
- Easy to mock in tests

### 3. Repository Pattern

VectorService abstracts Pinecone operations:

```python
# Instead of direct Pinecone calls
vector_service.upsert_vectors(vectors, metadatas)
vector_service.query_similar(vector, top_k)
```

Benefits:
- Can swap vector DB without changing business logic
- Consistent interface
- Easier testing

### 4. Custom Hook Pattern (React)

useChat hook encapsulates chat logic:

```typescript
const { messages, isLoading, sendMessage } = useChat();
```

Benefits:
- Reusable logic
- Separation of concerns
- Clean component code

## Technology Choices

### Why FastAPI?
- âœ… Async/await support
- âœ… Automatic OpenAPI docs
- âœ… Type validation with Pydantic
- âœ… High performance
- âœ… Easy dependency injection

### Why Pinecone?
- âœ… Serverless (no infrastructure)
- âœ… Fast similarity search
- âœ… Free tier available
- âœ… Simple API

### Why Llama 3?
- âœ… Strong instruction following
- âœ… Good context understanding
- âœ… Available via Hugging Face API
- âœ… Reasonable latency

### Why Sentence Transformers?
- âœ… High-quality embeddings
- âœ… Small model size (all-MiniLM-L6-v2)
- âœ… Fast inference
- âœ… Well-documented

### Why React + TypeScript?
- âœ… Type safety
- âœ… Large ecosystem
- âœ… Component reusability
- âœ… Great developer experience

### Why Radix UI?
- âœ… Accessible by default
- âœ… Unstyled (flexible styling)
- âœ… Composable primitives
- âœ… Well-maintained

### Why TailwindCSS?
- âœ… Utility-first approach
- âœ… Fast development
- âœ… Consistent design
- âœ… Small bundle size (with purge)

## Scalability Considerations

### Current Limitations

1. **Single Process**: Both frontend and backend run as single processes
2. **In-Memory State**: Service instances are in-memory
3. **No Caching**: Every query generates embeddings
4. **Rate Limits**: Hugging Face API has rate limits

### Scaling Strategies

**Horizontal Scaling:**
- Add load balancer
- Multiple FastAPI workers
- Session affinity not needed (stateless)

**Caching:**
- Redis for query embeddings
- Cache similar queries
- LRU cache for frequent questions

**Database:**
- PostgreSQL for user sessions
- Track query history
- Analytics

**Message Queue:**
- Celery for async indexing
- Background embedding generation
- Scheduled reindexing

**Monitoring:**
- Prometheus metrics
- Grafana dashboards
- Error tracking (Sentry)

## Security Considerations

### Current Implementation

âœ… Environment variables for secrets
âœ… CORS configuration
âœ… Input validation (Pydantic)
âœ… Error message sanitization

### Production Recommendations

1. **Authentication**: Add JWT tokens
2. **Rate Limiting**: Implement per-user limits
3. **HTTPS**: Use TLS in production
4. **API Key Rotation**: Rotate keys regularly
5. **Input Sanitization**: Additional validation
6. **Logging**: Audit logs for queries
7. **Secrets Management**: Use Vault or AWS Secrets Manager

## Performance Optimization

### Current Performance

- **Query latency**: 2-5 seconds (depends on LLM API)
- **Embedding generation**: ~100ms per query
- **Vector search**: ~50ms
- **Emotion detection**: ~200ms

### Optimization Opportunities

1. **Parallel Operations**: Run emotion detection + embedding in parallel
2. **Model Caching**: Cache model instances
3. **Batch Processing**: Process multiple queries together
4. **CDN**: Serve frontend from CDN
5. **Database Indexes**: Optimize Pinecone queries

## Testing Strategy

### Unit Tests

- Test each service independently
- Mock external APIs
- Test edge cases

### Integration Tests

- Test RAG pipeline end-to-end
- Test API endpoints
- Test error handling

### E2E Tests

- Test UI flows
- Test full query pipeline
- Test error states

## Future Enhancements

1. **Multi-language Support**: Translate queries and responses
2. **Voice Input**: Speech-to-text integration
3. **Document Upload**: Allow users to upload docs
4. **Conversation Memory**: Remember context across queries
5. **Admin Dashboard**: Monitor usage, performance
6. **A/B Testing**: Test different models/prompts
7. **Feedback Loop**: Learn from user feedback

---

This architecture is designed for **maintainability**, **scalability**, and **extensibility** while following industry best practices and SOLID principles.
